ClusterName: "default"

global:
  # Token: base64Encoded
  #  {
  #    "ot_endpoint": "https://auth_token_url",
  #    "ot_key": "key",
  #    "ot_secret": "secret",
  #    "ot_tenantId": "tenantId",
  #    "ot_metrics": "host:port",
  #    "ot_logs": "host:port",
  #    "tp_endpoint": "https://portal.opsramp.net",
  #    "tp_key": "key",
  #    "tp_secret": "secret",
  #    "tp_tenantId": "tenantId",
  #    "tp_metrics": "https://portal.opsramp.net",
  #    "tp_logs": "https://portal.opsramp.net",
  #    "tp_traces": "https://portal.opsramp.net"
  #  }
  Token: ""

metricsCollector:
  enabled: true
  replicas: 1
  scrape_configs_file: "scrape_configs.yaml"

logsCollector:
  enabled: true
  config:
    receivers:
      filelog:
        include:
          - /var/log/pods/*/*/*.log
        include_file_path: true
        include_file_name: true
        operators:
          # Find out which format is used by kubernetes
          - type: router
            id: get-format
            routes:
              - output: parser-docker
                expr: 'body matches "^\\{"'
              - output: parser-crio
                expr: 'body matches "^[^ Z]+ "'
              - output: parser-containerd
                expr: 'body matches "^[^ Z]+Z"'
          # Parse CRI-O format
          - type: regex_parser
            id: parser-crio
            regex: "^(?P<timestamp>[^ Z]+) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<message>.*)$"
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.timestamp
              layout_type: gotime
              layout: "2006-01-02T15:04:05.999999999Z07:00"
          # Parse CRI-Containerd format
          - type: regex_parser
            id: parser-containerd
            regex: "^(?P<timestamp>[^ ^Z]+Z) (?P<stream>stdout|stderr) (?P<logtag>[^ ]*) ?(?P<message>.*)$"
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.timestamp
              layout: "%Y-%m-%dT%H:%M:%S.%LZ"
          # Parse Docker format
          - type: json_parser
            id: parser-docker
            output: extract_metadata_from_filepath
            timestamp:
              parse_from: attributes.timestamp
              layout: "%Y-%m-%dT%H:%M:%S.%LZ"
          - type: move
            from: attributes.message
            to: body
          # Extract metadata from file path
          - type: regex_parser
            id: extract_metadata_from_filepath
            regex: '^.*\/(?P<namespace>[^_]+)_(?P<pod_name>[^_]+)_(?P<uid>[a-f0-9\-]{36})\/(?P<container_name>[^\._]+)\/(?P<restart_count>\d+)\.log$'
            parse_from: attributes["log.file.path"]
            cache:
              # default maximum amount of Pods per Node is 110
              size: 128
          # Rename attributes
          - type: move
            from: attributes.container_name
            to: resource["k8s.container.name"]
          - type: move
            from: attributes.namespace
            to: resource["k8s.namespace.name"]
          - type: move
            from: attributes.pod_name
            to: resource["k8s.pod.name"]

    processors:
      batch:
        timeout: 1s
        send_batch_size: 100
        send_batch_max_size: 500
      k8sattributes:
        passthrough: false
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
        extract:
          metadata:
            - k8s.cluster.uid
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
      transform:
        error_mode: ignore
        log_statements:
          - context: resource
            statements:
              - set(attributes["host"], attributes["k8s.node.name"])
              - set(attributes["type"], "event")
              - set(attributes["source"], "OpenTelemetry Collector")

    exporters:
      otlp/opsramp_logs:
        endpoint: '{{ include "otel.endpoint.logs" . | trim }}'
        headers:
          "tenantId": '{{ include "otel.auth.tenantId" . | trim }}'
        compression: "gzip"
        tls:
          insecure: false
          insecure_skip_verify: true
        auth:
          authenticator: oauth2client
        sending_queue:
          enabled: true
          num_consumers: 4
          queue_size: 1000
        retry_on_failure:
          enabled: true
          max_elapsed_time: 60s

    extensions:
      oauth2client:
        client_id: '{{ include "otel.auth.key" . | trim }}'
        client_secret: '{{ include "otel.auth.secret" . | trim }}'
        token_url: '{{ include "otel.auth.endpoint" . | trim }}'

    service:
      extensions:
        - oauth2client
      pipelines:
        logs:
          receivers: [ filelog ]
          processors: [ batch, k8sattributes, transform ]
          exporters: [ otlp/opsramp_logs ]

tracesCollector:
  enabled: true
  replicas: 1
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: '{{ default "0.0.0.0:4317" .Values.OpenTelemetryCollector.GRPC | trim }}'
          http:
            endpoint: '{{ default "0.0.0.0:4318" .Values.OpenTelemetryCollector.HTTP | trim }}'

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1000
        send_batch_max_size: 2000
      k8sattributes:
        passthrough: false
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
        extract:
          metadata:
            - k8s.cluster.uid
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
      transform:
        error_mode: ignore
        log_statements:
          - context: resource
            statements:
              - set(attributes["host"], attributes["k8s.node.name"])
              - set(attributes["type"], "event")
              - set(attributes["source"], "OpenTelemetry Collector")

    exporters:
      otlp/opsramp_traces:
        endpoint: '{{ index .Values "trace-proxy" "fullnameOverride" }}:9090'
        timeout: 30s
        tls:
          insecure: true
          insecure_skip_verify: true

    service:
      pipelines:
        traces:
          receivers: [ otlp ]
          processors: [ batch, k8sattributes, transform ]
          exporters: [ otlp/opsramp_traces ]

kubeEventsCollector:
  enabled: true
  config:
    receivers:
      k8s_events:
        auth_type: serviceAccount

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1000
        send_batch_max_size: 2000
      k8sattributes:
        passthrough: false
        pod_association:
          - sources:
              - from: resource_attribute
                name: k8s.pod.name
        extract:
          metadata:
            - k8s.cluster.uid
            - k8s.namespace.name
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.node.name
            - k8s.pod.start_time
            - k8s.deployment.name
            - k8s.replicaset.name
            - k8s.replicaset.uid
            - k8s.daemonset.name
            - k8s.daemonset.uid
            - k8s.job.name
            - k8s.job.uid
            - k8s.cronjob.name
            - k8s.statefulset.name
            - k8s.statefulset.uid
            - container.image.tag
            - container.image.name
      transform:
        error_mode: ignore
        log_statements:
          - context: resource
            statements:
              - set(attributes["host"], attributes["k8s.node.name"])
              - set(attributes["type"], "event")
              - set(attributes["source"], "OpenTelemetry Collector")

    exporters:
      debug:
        verbosity: detailed
        sampling_initial: 5
        sampling_thereafter: 200
      otlp/opsramp_logs:
        endpoint: '{{ include "otel.endpoint.logs" . | trim }}'
        headers:
          "tenantId": '{{ include "otel.auth.tenantId" . | trim }}'
        compression: "gzip"
        tls:
          insecure: false
          insecure_skip_verify: true
        auth:
          authenticator: oauth2client

    extensions:
      oauth2client:
        client_id: '{{ include "otel.auth.key" . | trim }}'
        client_secret: '{{ include "otel.auth.secret" . | trim }}'
        token_url: '{{ include "otel.auth.endpoint" . | trim }}'

    service:
      extensions:
        - oauth2client
      pipelines:
        logs:
          receivers: [ k8s_events ]
          processors: [ batch, k8sattributes, transform ]
          exporters: [ otlp/opsramp_logs, debug ]

kubeStateMetrics:
  enabled: true

nodeExporter:
  enabled: true


## OpenTelemetry Collector Endpoint
OpenTelemetryCollector:
  GRPC: "0.0.0.0:4317"
  HTTP: "0.0.0.0:4318"

trace-proxy:
  fullnameOverride: "proxy"
  replicaCount: 1
  image:
    repository: ghcr.io/lokeshopsramp/trace-proxy
    pullPolicy: "IfNotPresent"
    tag: "arm"
  config:
    OpsrampAPI: '{{ include "tp.endpoint.traces" . | trim }}'
    AuthConfiguration:
      Endpoint: '{{ include "tp.auth.endpoint" . | trim }}'
      Key: '{{ include "tp.auth.key" . | trim }}'
      Secret: '{{ include "tp.auth.secret" . | trim }}'
      TenantId: '{{ include "tp.auth.tenantId" . | trim }}'
    LogsEndpoint: '{{ include "tp.endpoint.logs" . | trim }}'
    MetricsConfig:
      Enable: true
      OpsRampAPI: '{{ include "tp.endpoint.metrics" . | trim }}'


opentelemetry-operator:
  nameOverride: opentelemetry-operator
  admissionWebhooks:
    certManager:
      enabled: true

  ## Provide OpenTelemetry Operator manager container image and resources.
  ##
  manager:
    image:
      repository: ghcr.io/open-telemetry/opentelemetry-operator/opentelemetry-operator
      tag: ""
    collectorImage:
      repository: otel/opentelemetry-collector-contrib
      tag: 0.97.0
    opampBridgeImage:
      repository: ""
      tag: ""
    targetAllocatorImage:
      repository: ""
      tag: ""
    autoInstrumentationImage:
      java:
        repository: ""
        tag: ""
      nodejs:
        repository: ""
        tag: ""
      python:
        repository: ""
        tag: ""
      dotnet:
        repository: ""
        tag: ""
      # The Go instrumentaiton support in the operator is disabled by default.
      # To enable it, use the operator.autoinstrumentation.go feature gate.
      go:
        repository: ""
        tag: ""
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 64Mi
